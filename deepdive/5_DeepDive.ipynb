{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **커널 기법**"
      ],
      "metadata": {
        "id": "wtl9ktCSx_a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM은 비선형적인 문제에 대해서도 해결책을 제시합니다. 선형적으로 분리되지 않는 데이터를 다룰 때 커널 기법을 활용하여 고차원 공간으로 변환함으로써 분류를 가능하게 합니다. 대표적인 커널에는 선형 커널, 다항식 커널, RBF 커널  등이 있습니다.\n",
        "- 다항식 커널과 RBF 커널의 차이는 무엇인가? 각각의 정확한 개념과  둘 사이의 유사점, 차이점은 무엇인가?\n",
        "- 다항식 커널과 RBF 커널의 하이퍼파라미터 각각 d와 γ은 커질수록 오버피팅을 야기할 수 있다. 그 이유는 무엇인가?"
      ],
      "metadata": {
        "id": "wVUjQYB-yet4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **다항식 커널과 RBF 커널의 차이는 무엇인가? / 각각의 정확한 개념과 둘 사이의 유사점, 차이점은 무엇인가?**"
      ],
      "metadata": {
        "id": "VXuchiem0EPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "커널은 선형적으로 분리되지 않는 데이터를 고차원 공간으로 매핑하여, 데이터를 선형적으로 분리 가능하게 만드는 역할을 합니다.\n",
        "\n",
        "**다항식 커널**\n",
        "$$K(x, y) = (x\\cdot y + c)^{d}$$\n",
        "\n",
        "**RBF 커널**\n",
        "$$K(x, y) = exp(-\\gamma||x-y||^{2})$$\n",
        "\n",
        "* 유사점\n",
        "  * 커널 함수: 커널 트릭을 사용하여 데이터 포인트를 고차원 매핑\n",
        "  * 비선형 모델링\n",
        "  * 하이퍼파라미터\n",
        "* 차이점\n",
        "  * 모델링 방식(수학적 표현)"
      ],
      "metadata": {
        "id": "E5u5bKnNDCGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **다항식 커널과 RBF 커널의 하이퍼파라미터 각각 d와 γ은 커질수록 오버피팅을 야기할 수 있다. 그 이유는 무엇인가?**"
      ],
      "metadata": {
        "id": "KW-NErUYDKhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**왜 d(다항식의 차수)가 커질수록 오버피팅이 발생할까?**\n",
        "* 복잡한 결정 경계\n",
        "* 훈련 데이터의 노이즈 반영\n",
        "\n",
        "**왜 γ(두 데이터 포인트 사이의 거리)가 커질수록 오버피팅이 발생할까?**\n",
        "* 과도한 민감도\n",
        "  * 작은 거리에서 큰 영향\n",
        "  * 경계의 과도한 조정"
      ],
      "metadata": {
        "id": "mZjY0jufDOga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **가장 좋은 하이퍼파라미터**"
      ],
      "metadata": {
        "id": "LS-nyhPAycbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 SVM을 공부하면서 γ와 C라는 하이퍼파라미터에 대해 공부했습니다. 우리는 머신러닝, 딥러닝을 공부하며 더 많은 하이퍼파라미터들을 마주하게 됩니다. 하이퍼파라미터는 개인의 주관에 따라 적용이 가능하지만, 동시에 모델의 성능에 큰 영향을 끼칩니다. 그렇다면 우리는 어떻게 가장 좋은 하이퍼파라미터를 찾을 수 있을까요? 수많은 숫자를 하나하나 대입해보아야 할까요?\n",
        "- 하이퍼파라미터를 튜닝하는 전통적인 방식에는 어떤 방법들이 있으며 이들은 어떤 로직으로 작동할까?\n",
        "- AutoML이란 무엇이고, 이것이 항상 전통적인 방식의 하이퍼파라미터 튜닝보다 좋을까?\n",
        "- 항상 그렇지 않다면 언제 그런지, 그리고 그 이유는 무엇일까?"
      ],
      "metadata": {
        "id": "mi72Z3TAyiVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **하이퍼파라미터를 튜닝하는 전통적인 방식에는 어떤 방법들이 있으며 이들은 어떤 로직으로 작동할까?**"
      ],
      "metadata": {
        "id": "zN7gj5xbDSpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **그리드 서치**\n",
        "  * 로직: 주어진 하이퍼파라미터들의 가능한 모든 조합을 탐색합니다. 예를 들어, 모델의 하이퍼파라미터 $\\alpha$와 $\\beta$가 각각\n",
        "{0.01,0.1}과 {10,100}일 때, 그리드 서치는 총 4가지 조합([0.01, 10], [0.01, 100], [0.1, 10], [0.1, 100])에 대해 모델을 훈련하고 성능을 평가합니다.\n",
        "2. **랜덤 서치**\n",
        "  * 로직: 하이퍼파라미터 공간에서 무작위로 하이퍼파라미터 값을 선택하여 모델을 훈련시킵니다. 지정된 횟수만큼 무작위로 샘플을 선택하여 탐색합니다.\n",
        "3. **베이지안 최적화**\n",
        "  * 로직: 이전의 하이퍼파라미터 실험 결과를 바탕으로 다음 실험을 결정하는 방법입니다. 이 과정은 주로 확률 모델(예: 가우시안 프로세스)을 사용하여 성능을 예측하고, 효율적으로 하이퍼파라미터 공간을 탐색합니다.\n",
        "4. **진화 알고리즘**\n",
        "  * 로직: 자연 선택을 모방한 방법입니다. 여러 하이퍼파라미터의 세트를 '개체'로 보고, 이를 교차하고 돌연변이를 일으키면서 하이퍼파라미터 공간을 탐색합니다.\n"
      ],
      "metadata": {
        "id": "qU8c3xGADTFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AutoML이란 무엇이고, 이것이 항상 전통적인 방식의 하이퍼파라미터 튜닝보다 좋을까? / 항상 그렇지 않다면 언제 그런지, 그리고 그 이유는 무엇일까?**"
      ],
      "metadata": {
        "id": "3aJoGNPgDTKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoML이란 무엇인가?**\n",
        "```\n",
        "AutoML은 머신러닝 모델을 설계하고, 훈련하며, 최적화하는 과정의 대부분을 자동화하는 기술입니다.\n",
        "```\n",
        "**AutoML과 전통적인 하이퍼파라미터 튜닝의 차이점**\n",
        "```\n",
        "AutoML은 하이퍼파라미터 튜닝뿐만 아니라, 모델 선택, 데이터 전처리, 특성 엔지니어링 등 모든 과정이 자동화되어 있습니다.\n",
        "(예: 모델 선택, 하이퍼파라미터 튜닝, 특성 엔지니어링, 교차 검증 및 평가 등)\n",
        "```\n",
        "**AutoML이 전통적인 방식의 하이퍼파라미터 튜닝보다 항상 좋을까?**\n",
        "```\n",
        "AutoML은 대부분의 경우 시간 절약과 편리함을 제공하며, 특히 모델링 경험이 부족한 사람들에게 유용합니다.\n",
        "효율적이고 자동화된 방식으로 모델을 최적화하고, 하이퍼파라미터 튜닝을 자동화할 수 있어,\n",
        "특히 빠르게 프로토타입을 만들고 실험해야 할 때 매우 유용합니다.\n",
        "\n",
        "하지만, 세밀한 조정이나 도메인 특화 문제에서는 전통적인 방식이 더 효과적일 수 있습니다.\n",
        "전통적인 방법은 사용자가 모델을 더 깊이 이해하고 그에 맞게 맞춤형 튜닝을 할 수 있는 장점이 있기 때문입니다.\n",
        "따라서, AutoML은 일반적인 작업에 매우 유용하지만, 복잡한 문제나 도메인 지식이 중요한 경우에는\n",
        "여전히 전통적인 하이퍼파라미터 튜닝 방식이 더 나을 수 있습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "x6rc7-9XIKUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **나무를 잘 기르는 법**"
      ],
      "metadata": {
        "id": "yItW2JSDyiXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "결정 트리는 매우 훌륭합니다! 분류, 회귀문제를 모두 해결할 수 있고, 결측치에 대처가 쉽고 스케일링에서도 상대적으로 자유로운, 좋은 성질을 많이 가진 모델입니다. 특히나 해석이 쉽게 가능하다는 점과 이 모델의 로직을 생각해보면 단순해 보이기도 합니다.  하지만 최적의 결정트리를 만들기 위해서는 더 고려할 점들이 있습니다.\n",
        "- 나무의 깊이는 오버피팅과 직결된다. 이를 방지하기 위한 방식 중 프루닝은 무엇이며, 프리 프루닝과 포스트 프루닝의 차이는 무엇인가?\n",
        "- 앙상블 기법이 무엇인가? 이 기법이 결정트리에 끼치는 효용과, 그 결과로 볼 수 있는 모델들을 살펴보자.\n",
        "- 탐욕 알고리즘은 트리 계열 모델의 핵심이다. 그렇다면 다른 머신러닝 모델들은 탐욕 알고리즘을 사용하지 않을까? 어떤 모델들이 탐욕 알고리즘을 사용하는지, 그 이유와 특징을 살펴보자."
      ],
      "metadata": {
        "id": "2iWGSeYjyiZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **나무의 깊이는 오버피팅과 직결된다. 이를 방지하기 위한 방식 중 프루닝은 무엇이며, 프리 프루닝과 포스트 프루닝의 차이는 무엇인가?**"
      ],
      "metadata": {
        "id": "OuROqEuRyich"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**프루닝이란?**\n",
        "```\n",
        "프루닝은 주로 결정 트리와 같은 트리 기반 모델에서 사용되는 기법으로,\n",
        "모델의 복잡도를 줄이고 과적합을 방지하기 위해 트리의 일부 노드나 가지를 제거하는 과정입니다.\n",
        "```\n",
        "**프리 프루닝 vs. 포스트 프루닝**\n",
        "* 프리 프루닝은 트리 생성 도중에 트리의 성장 자체를 제한하여 과적합을 방지하려는 방식입니다. 초기 단계에서 모델을 제한하므로 계산이 빨라지지만, 지나치게 제한적일 수 있습니다.\n",
        "* 포스트 프루닝은 트리 생성 후에 불필요한 가지를 제거하는 방식으로, 더 정확한 모델을 얻을 수 있지만 계산이 더 복잡하고 시간이 오래 걸릴 수 있습니다."
      ],
      "metadata": {
        "id": "S36DSB2kJ3pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **앙상블 기법이 무엇인가? 이 기법이 결정트리에 끼치는 효용과, 그 결과로 볼 수 있는 모델들을 살펴보자.**"
      ],
      "metadata": {
        "id": "IyjPdBgIKbdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**앙상블 기법이란?**\n",
        "```\n",
        "앙상블 기법은 여러 개의 모델을 결합하여 더 강력하고 안정적인 예측을 도출하는 머신러닝 기법입니다.\n",
        "앙상블 기법의 기본 아이디어는 여러 약한 모델을 결합하여, 더 강력한 모델을 만드는 것입니다.\n",
        "```\n",
        "**앙상블의 장단점(효용)**\n",
        "* **장점**\n",
        "  * 과적합 감소\n",
        "  * 성능 향상\n",
        "  * 안정성 증가\n",
        "* **단점**\n",
        "  * 계산 비용\n",
        "  * 과도한 모델 복잡도(해석의 어려움)\n",
        "\n",
        "**앙상블 기법의 예시(결과)**\n",
        "* 랜덤 포레스트\n",
        "* AdaBoost\n",
        "* XGBoost"
      ],
      "metadata": {
        "id": "6Ln4c262LEDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **탐욕 알고리즘은 트리 계열 모델의 핵심이다. 그렇다면 다른 머신러닝 모델들은 탐욕 알고리즘을 사용하지 않을까? 어떤 모델들이 탐욕 알고리즘을 사용하는지, 그 이유와 특징을 살펴보자.**"
      ],
      "metadata": {
        "id": "RFSOHHCGLike"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**탐욕 알고리즘**\n",
        "```\n",
        "탐욕 알고리즘은 주어진 문제를 해결하기 위해 각 단계에서 최선의 선택을 하는 방식입니다.\n",
        "즉, 각 단계에서 가장 좋은 선택을 하는 것이 전체적으로 최적의 해를 만들 수 있다고 가정하고,\n",
        "다음 선택을 할 때마다 최적이라고 판단되는 선택을 합니다.\n",
        "```\n",
        "**탐욕 알고리즘을 사용하는 머신러닝 모델**\n",
        "* K-최근접 이웃\n",
        "  * 알고리즘: KNN은 예측을 위해 가장 가까운 K개의 이웃을 탐색하는 과정에서 탐욕적으로 가장 가까운 이웃을 선택하여 예측합니다.\n",
        "  * 이유: KNN은 예측을 위해 주어진 데이터와 가장 가까운 데이터를 선택하는 방식으로, 빠르게 예측을 할 수 있습니다.\n",
        "* 그라디언트 부스팅\n",
        "  * 알고리즘: 그라디언트 부스팅에서는 각 모델이 예측 오류를 보정하는 과정에서 가장 좋은 모델을 순차적으로 선택합니다. 이때 각 모델은 최적의 방향으로 가는 탐욕적 선택을 통해 예측을 개선합니다.\n",
        "  * 이유: 그라디언트 부스팅은 각 단계에서의 오류를 최소화하는 방향으로 학습이 이루어집니다. 각 단계에서 최선의 예측을 위해 탐욕적으로 가장 큰 기여를 할 수 있는 모델을 선택합니다.\n",
        "* 히스토그램 기반 모델\n",
        "  * 알고리즘 : 히스토그램 기반 모델(예: LightGBM)은 리프 기준 성장 방식을 사용하며, 각 단계에서 가장 유효한 분할을 탐욕적으로 선택하여 모델을 생성합니다.\n",
        "  * 이유: 데이터가 많은 경우, 효율적인 학습을 위해 탐욕적 선택을 사용하여 빠르게 최적 분할을 찾아내는 방식입니다."
      ],
      "metadata": {
        "id": "dzin_uJ-MhmI"
      }
    }
  ]
}