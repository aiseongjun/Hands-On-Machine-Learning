{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **릿지 회귀와 라쏘 회귀**\n",
        "릿지 회귀와 라쏘 회귀는 선형 회귀의 규제된 버전입니다.\n",
        "릿지 회귀와 라쏘 회귀 모두 비용 함수에 규제항을 더하지만, 라쏘 회귀의 경우 L2 노름 대신 가중치 벡터의 L1 노름을 사용합니다.\n",
        "* 어떤 경우에 릿지 회귀가 유리하며 어떤 경우에 라쏘 회귀가 유리한가?\n",
        "* 데이터의 크기와 차원이 커질 경우 둘의 성능과 효율성은 어떻게 변화할까?\n",
        "* 엘라스틱넷은 둘의 어떠한 장점과 특징을 각각 가져가는가?"
      ],
      "metadata": {
        "id": "1D4oAaRt9P2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **어떤 경우에 릿지 회귀가 유리하며 어떤 경우에 라쏘 회귀가 유리한가?**"
      ],
      "metadata": {
        "id": "cyXOThPo9Wn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**릿지 회귀가 유리한 경우**\n",
        "1. 모든 변수들이 중요할 때\n",
        "  * 릿지 회귀는 모든 가중치를 축소시키지만, 완전히 0으로 만들지 않습니다. 따라서 모든 변수가 중요한 정보를 가지고 있는 경우 유리합니다.\n",
        "2. 변수들 간의 상관관계가 높을 때\n",
        "  * 변수들 간에 상호작용이 존재하거나, 다중공선성이 있는 경우 유리합니다.\n",
        "**라쏘 회귀가 유리한 경우**\n",
        "1. 특성 선택이 필요할 때\n",
        "  * 라쏘 회귀는 일부 가중치를 0으로 만들기 때문에, 불필요한 변수를 자동으로 제거할 수 있습니다. 즉, 모델에 중요한 변수만 남기고 특성을 선택할 수 있습니다.\n",
        "2. 희소 모델이 필요할 때\n",
        "  * 일부 특성만을 사용하여 모델의 해석을 용이하게 하고 싶을 경우 유리합니다.\n",
        "3. 변수들 간의 상관관계가 낮을 때\n",
        "  * 변수들 간의 상관관계가 낮아, 몇몇 변수를 선택하고 나머지를 버리고 싶을 경우 유리합니다."
      ],
      "metadata": {
        "id": "E0NDZEq9YaV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **데이터의 크기와 차원이 커질 경우 둘의 성능과 효율성은 어떻게 변화할까?**"
      ],
      "metadata": {
        "id": "GGAJoxPt95jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**릿지 회귀의 성능과 효율성**\n",
        "* 성능\n",
        "  * L2 정규화는 모든 계수를 균등하게 축소하지만 0으로 만들지 않습니다. 변수 간의 상관관계가 높은 경우 안정적이지만, 고차원에서는 모든 변수를 유지하므로 과적합 위험이 증가할 수 있습니다.\n",
        "* 효율성\n",
        "  * 릿지 회귀는 해석이 간단하고 계산적으로 효율적입니다. 하지만 데이터의 크기가 커지면 계산 시간이 늘어날 수 있습니다.\n",
        "\n",
        "**라쏘 회귀의 성능과 효율성**\n",
        "* 성능\n",
        "  * L1 정규화는 불필요한 피처의 계수를 0으로 축소하여 모델을 생성합니다. 실제로 관련성이 낮은 변수가 많을 경우 예측 성능이 우수합니다. 하지만 차원이 너무 커지면, 과도하게 많은 계수가 0으로 설정될 수 있으며, 중요한 변수까지 제외되는 리스크도 존재합니다.\n",
        "* 효율성\n",
        "  * 라쏘 회귀는 변수 선택을 하는 과정에서 계산이 복잡해질 수 있습니다. 수렴 속도가 느려질 수 있으며, 데이터가 많아지면 계산이 비효율적일 수 있습니다."
      ],
      "metadata": {
        "id": "WMai6Aq395li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **엘라스틱넷은 둘의 어떠한 장점과 특징을 각각 가져가는가?**"
      ],
      "metadata": {
        "id": "_f5vSA7Qajfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**엘라스틱넷의 장점과 특징**\n",
        "* 릿지 회귀의 장점 (L2 정규화):\n",
        "  * 과적합 방지: 릿지처럼, 엘라스틱넷은 계수의 크기를 제한하여 과적합을 방지할 수 있습니다.\n",
        "  * 다중공선성 문제 해결: 릿지처럼, 엘라스틱넷은 변수 선택을 강제로 하지 않기 때문에, 변수를 모두 유지하면서 모델을 학습할 수 있습니다. 이로 인해 변수들 간의 상관 관계가 강할 때 유리합니다.\n",
        "* 라쏘 회귀의 장점 (L1 정규화):\n",
        "  * 변수 선택: 라쏘처럼 엘라스틱넷은 L1 정규화를 사용하여 불필요한 변수의 계수를 0으로 만들 수 있습니다. 이로 인해 중요한 변수만을 선택해 모델을 간소화할 수 있습니다.\n",
        "  * 희소성: 라쏘처럼, 엘라스틱넷은 계수 중 일부를 정확히 0으로 만들어 희소한 모델을 생성할 수 있어, 변수 선택에 유리합니다."
      ],
      "metadata": {
        "id": "276YZEpiajmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **미니배치 경사 하강법의 배치 크기**\n",
        "미니배치 경사 하강법은 각 스텝에서 전체 훈련 세트나 하나의 샘플 기반으로 그레이디언트를 계산하는 것이 아니라 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산합니다. 이때 미니배치 경사 하강법의 배치 크기를 설정하는 것은 아주 중요한 문제입니다.\n",
        "* 배치 크기가 모델 학습에 미치는 영향은 무엇일까?\n",
        "* 어떤 경우에 큰 배치가 유리하고 어떤 경우에 작은 배치가 유리한가?\n",
        "* 여러 배치 크기를 일일이 실험해보는 것 없이 효율적으로 최적화된 배치 크기를 찾는 방법은 없을까?"
      ],
      "metadata": {
        "id": "eCrLTndE95n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **배치 크기가 모델 학습에 미치는 영향은 무엇일까?**"
      ],
      "metadata": {
        "id": "FX-bZ0zJ98sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 크기는 모델 학습에서 중요한 하이퍼파라미터 중 하나로, 모델이 데이터를 처리하고 학습하는 방식에 영향을 미칩니다. 배치 크기가 모델 학습에 미치는 주요 영향은 학습 속도, 일반화 성능, 메모리 사용, 수렴 속도, 최적화 등 다양한 측면에서 다르게 나타납니다."
      ],
      "metadata": {
        "id": "A83KHITS-btN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **어떤 경우에 큰 배치가 유리하고 어떤 경우에 작은 배치가 유리한가?**"
      ],
      "metadata": {
        "id": "rBKkv_-Meub0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**큰 배치가 유리한 경우**\n",
        "* 하드웨어 자원이 충분한 경우\n",
        "* 빠른 수렴이나 안정적인 업데이트가 필요한 경우\n",
        "* 병렬 처리나 분산 학습 환경\n",
        "\n",
        "**작은 배치가 유리한 경우**\n",
        "* 메모리가 제한적일 때\n",
        "* 일반화 성능을 중시할 때(노이즈를 이용해 더 좋은 최적화)\n",
        "* 실시간 학습 또는 온라인 학습(고속 업데이트가 필요할 때)"
      ],
      "metadata": {
        "id": "tsT6cRbiew0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **여러 배치 크기를 일일이 실험해보는 것 없이 효율적으로 최적화된 배치 크기를 찾는 방법은 없을까?**"
      ],
      "metadata": {
        "id": "r3rjhP02exH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 배치 크기와 학습률의 관계를 활용한 스케일링\n",
        "  * 배치 크기와 학습률은 밀접하게 연관되어 있습니다. 일반적으로 배치 크기가 커질수록 학습률도 함께 증가시킬 수 있습니다.\n",
        "* 동적 배치 크기 조정\n",
        "  * 학습 초기에는 작은 배치 크기를 사용하여 안정적인 학습을 시작하고, 모델이 점차 안정화되면 배치 크기를 점진적으로 늘려가는 방식입니다.\n",
        "* 하이퍼파라미터 최적화 기법\n",
        "  * 여러 배치 크기를 그리드 탐색, 랜덤 탐색, 베이지안 최적화 등을 사용하여 최적화할 수 있습니다."
      ],
      "metadata": {
        "id": "Y8hxg6GSexak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **소프트맥스와 손실 함수**\n",
        "회귀 모델은 모델 별로 다양한 손실 함수가 사용됩니다. 소프트맥스 회귀의 경우 크로스 엔트로피 손실 함수를 사용하여 학습을 진행하는데요, 크로스 엔트로피는 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는지 측정하는 용도로 사용됩니다.\n",
        "$$Cross-Entropy(y, \\hat{y}) = -\\sum y_{i}\\log(\\hat{y})$$\n",
        "* 손실 함수에는 어떤 것들이 있을까?\n",
        "* 크로스 엔트로피 손실 함수의 직관적인 해석은?\n",
        "* 왜 소프트맥스와 크로스 엔트로피를 결합하는 것이 최적화에 유리한가?"
      ],
      "metadata": {
        "id": "MIYHVlec-bve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **손실 함수에는 어떤 것들이 있을까?**"
      ],
      "metadata": {
        "id": "kvrJX7c8-dTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**회귀 문제**\n",
        "* 평균 제곱 오차\n",
        "* 평균 절대 오차\n",
        "* 허브 손실\n",
        "* 평균 제곱 대근사 오차\n",
        "\n",
        "**분류 문제**\n",
        "* 교차 엔트로피 손실\n",
        "* 힌지 손실\n",
        "* Sparse Categorical Cross-Entropy\n",
        "* Kullback-Leibler Divergence (KL Divergence)\n",
        "\n",
        "**기타 손실 함수**\n",
        "* Triplet Loss\n",
        "* Contrastive Loss\n",
        "* Focal Loss"
      ],
      "metadata": {
        "id": "GeBgE7wNe27a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **크로스 엔트로피 손실 함수의 직관적인 해석은?**"
      ],
      "metadata": {
        "id": "kqgEYJuDe3Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 모델이 정확한 예측을 했다면 손실은 매우 작습니다. 반대로 모델이 잘못된 예측을 했다면 손실은 커집니다."
      ],
      "metadata": {
        "id": "L_1WgwECe5FA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **왜 소프트맥스와 크로스 엔트로피를 결합하는 것이 최적화에 유리한가?**"
      ],
      "metadata": {
        "id": "GzmRUQxVe5Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 수치적 안정성\n",
        "* 효율적 그래디언트 계산"
      ],
      "metadata": {
        "id": "PigWXJGte6qX"
      }
    }
  ]
}