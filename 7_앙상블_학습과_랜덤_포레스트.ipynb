{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSo9Q3o6lP2r52s27U46MA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiseongjun/Hands-On-Machine-Learning/blob/main/7_%EC%95%99%EC%83%81%EB%B8%94_%ED%95%99%EC%8A%B5%EA%B3%BC_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 앙상블 학습과 랜덤 포레스트**"
      ],
      "metadata": {
        "id": "ICRrz8nSl6VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**앙상블 학습이란?**\n",
        "```\n",
        "앙상블 학습은 여러 개의 모델을 결합하여 하나의 강력한 예측 모델을 만드는 기법입니다.\n",
        "각 개별 모델이 갖는 약점을 보완하고, 전체적으로 더 나은 예측 성능을 얻기 위해 여러 모델을 사용하는 방식입니다.\n",
        "앙상블 학습은 보통 약한 학습기를 결합하여 강한 학습기를 만드는 데 사용됩니다(큰 수의 법칙).\n",
        "```"
      ],
      "metadata": {
        "id": "IHcxNoSCmvSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.1 튜표 기반 분류기**"
      ],
      "metadata": {
        "id": "81ntp8LXmOyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**직접 튜표 분류기**\n",
        "```\n",
        "더 좋은 분류기를 만드는 매우 간단한 방법은 각 분류기의 예측을 집계하는 것입니다.\n",
        "직접 튜표 분류기는 다수결 튜표로 정해지는 분류기로, 가장 많은 표를 얻은 클래스가 앙상블의 예측이 됩니다.\n",
        "```\n",
        "**앙상블 모델의 정확도를 향상시키는 방법**\n",
        "```\n",
        "앙상블 방법은 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘합니다.\n",
        "다양한 분류기를 얻는 한 가지 방법은 각기 다른 알고리즘으로 학습시키는 것입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "679AetFymukI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators = [\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(random_state=42))\n",
        "    ]\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "print('clf.predict:\\n {}\\n'.format([clf.predict(X_test[:1]) for clf in voting_clf.estimators_]))\n",
        "\n",
        "for name, clf in voting_clf.named_estimators_.items():\n",
        "  print(name, '=', clf.score(X_test, y_test))\n",
        "print('voting_clf.score: {}'.format(voting_clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaXOPYXNqBpi",
        "outputId": "c69abd44-c7c7-46e7-9896-39f76b92c13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf.predict:\n",
            " [array([1]), array([1]), array([0])]\n",
            "\n",
            "lr = 0.864\n",
            "rf = 0.896\n",
            "svc = 0.896\n",
            "voting_clf.score: 0.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**간접 튜표 분류기**\n",
        "```\n",
        "모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있습니다.\n",
        "간접 튜표 분류기는 이 방식을 사용해서 확률이 높은 튜표에 비중을 더 두기 때문에 직접 튜표 방식보다 성능이 높습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "1PIKPRF9tbq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.voting='soft'\n",
        "voting_clf.named_estimators['svc'].probability = True\n",
        "voting_clf.fit(X_train, y_train)\n",
        "print('voting_clf.score: {}'.format(voting_clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80C7jooouy6g",
        "outputId": "972c5292-9d43-4a87-c56a-a8167483862f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voting_clf.score: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.2 배깅과 페이스팅**"
      ],
      "metadata": {
        "id": "Crt3lsxLvAq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**배깅과 페이스팅**\n",
        "```\n",
        "다양한 분류기를 만드는 다른 방법은 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습시키는 것입니다.\n",
        "훈련 세트에서 중복을 허용하여 샘플링하는 방식을 배깅, 중복을 허용하지 않는 방식을 페이스팅이라고 합니다.\n",
        "```\n",
        "**집계 함수**\n",
        "```\n",
        "집계 함수는 일반적으로 분류일 때는 통계적 최빈값을, 회귀에 대해서는 평균을 계산합니다.\n",
        "일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 줄어듭니다.\n",
        "```"
      ],
      "metadata": {
        "id": "K7KpcO62vOV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2.1 사이킷런의 배깅과 페이스팅**"
      ],
      "metadata": {
        "id": "fD2phmZ3wwIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "                            max_samples=100, n_jobs=-1, random_state=42).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ciLOqxVs0Oob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2.2 OOB 평가**"
      ],
      "metadata": {
        "id": "Q2jTfWBb03r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OOB(out-of-bag)**\n",
        "```\n",
        "배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링되고 어떤 것은 전혀 선택되지 않을 수 있습니다.\n",
        "선택되지 않은 나머지를 OOB 샘플이라고 부릅니다.\n",
        "예측기가 훈련되는 동안에는 OOB 샘플을 사용하지 않으므로 OOB 샘플을 사용해 평가할 수 있습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "A0WPDwXn1NV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "                            oob_score=True, n_jobs=1, random_state=42).fit(X_train, y_train)\n",
        "print('bag_clf.oob_score_: {}'.format(bag_clf.oob_score_))\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print('accuracy_score: {}'.format(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "print('\\nbag_clf.oob_decision_function_[:3]:\\n {}'.format(bag_clf.oob_decision_function_[:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqOoTdYC2no3",
        "outputId": "c2d4d50e-8f14-44a3-c771-4ccc9c0140b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag_clf.oob_score_: 0.896\n",
            "accuracy_score: 0.92\n",
            "\n",
            "bag_clf.oob_decision_function_[:3]:\n",
            " [[0.32352941 0.67647059]\n",
            " [0.3375     0.6625    ]\n",
            " [1.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.3 랜덤 패치와 랜덤 서브스페이스**"
      ],
      "metadata": {
        "id": "D3sDUZdA3BzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**랜덤 패치와 랜덤 서브스페이스**\n",
        "```\n",
        "BaggingClassifier는 특성 샘플링도 지원합니다.\n",
        "각 예측기는 랜덤으로 선택한 입력 특성의 일부분으로 훈련됩니다.\n",
        "훈련 특성과 샘플을 모두 샘플링하는 것을 랜덤 패치 방식이라 하고\n",
        "훈련 샘플을 모두 사용하고 특성을 샘플링하는 것을 랜덤 서브스페이스 방식이라고 합니다.\n",
        "이 기법은 훈련 속도를 크게 높일 수 있어 고차원의 데이터셋을 다룰 때 유용합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "Rl6Isk9d3z4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.4 랜덤 포레스트**"
      ],
      "metadata": {
        "id": "dHmVf4n65P61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**랜덤 포레스트**\n",
        "```\n",
        "랜덤 포레스트는 일반적으로 배깅 방법(또는 페이스팅)을 적용한 결정 트리의 앙상블입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "A8dcLr115WGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(max_features='sqrt', max_leaf_nodes=16),\n",
        "    n_estimators=500, n_jobs=-1, random_state=42)\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
        "                                 n_jobs=-1, random_state=42).fit(X_train, y_train)\n",
        "print('rnd_clf.predict:\\n {}'.format(rnd_clf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmDnlAe5bC1j",
        "outputId": "d2d349c1-83cf-425b-d903-7cafa4e62d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnd_clf.predict:\n",
            " [0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.4.1 엑스트라 트리**"
      ],
      "metadata": {
        "id": "upZtOM-Obhpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**익스트림 랜덤 트리(엑스트라 트리)**\n",
        "```\n",
        "트리를 더욱 랜덤하게 만들기 위해 최적의 임곗값을 찾는 대신 후보 특성을 사용해 랜덤으로 분할한 다음 그중에서 최상의 분할을 선택합니다.\n",
        "이와 같이 극단적으로 랜덤한 트리의 랜덤 포레스트를 익스트림 랜덤 트리 앙상블이라고 부릅니다.\n",
        "```"
      ],
      "metadata": {
        "id": "UiT_VTeoeP4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.4.2 특성 중요도**"
      ],
      "metadata": {
        "id": "n3BxU79XdMO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**특성 중요도**\n",
        "```\n",
        "사이킷런은 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정합니다.\n",
        "더 정확하게는 가중치 평균이며, 각 노드의 가중치는 연관된 훈련 샘플 수와 같습니다.\n",
        "사이킷런은 훈련이 끝난 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결괏값을 정규화합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "k8iLGbOofPfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42).fit(iris.data, iris.target)\n",
        "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
        "  print(round(score, 2), name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l6mIrlsgsAm",
        "outputId": "e8a44cff-f9b3-4aff-8fa7-597364a931fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11 sepal length (cm)\n",
            "0.02 sepal width (cm)\n",
            "0.44 petal length (cm)\n",
            "0.42 petal width (cm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.5 부스팅**"
      ],
      "metadata": {
        "id": "Zeopl96JhAEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**부스팅**\n",
        "```\n",
        "부스팅은 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법을 말합니다.\n",
        "부스팅 방법의 아이디어는 앞의 모델을 보완해 나가면서 일련의 예측기를 학습시키는 것입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "oKcllPHYhE9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.5.1 AdaBoost**"
      ],
      "metadata": {
        "id": "Q-jwRZjnhloh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost**\n",
        "```\n",
        "이전 예측기를 보완하는 새로운 예측기를 만드는 방법은 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것입니다.\n",
        "이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됩니다.\n",
        "이것이 AdaBoost에서 사용하는 방식입니다.\n",
        "```\n",
        "[식 7-4] AdaBoost 예측\n",
        "$$\\hat{y}(X) = \\underset{k}{argmax}\\sum^{N}_{j=1}\\alpha_{j} (\\hat{y}_j(X) = k)$$\n",
        "* $N$은 예측기 수"
      ],
      "metadata": {
        "id": "UiViT-r_kWMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
        "    learning_rate=0.5, random_state=42).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PZxRZnx6kl1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.5.2 그레이디언트 부스팅**"
      ],
      "metadata": {
        "id": "Ftdl76NsM8MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**그레이디언트 부스팅**\n",
        "```\n",
        "그레이디언트 부스팅은 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가합니다.\n",
        "하지만 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킵니다.\n",
        "```"
      ],
      "metadata": {
        "id": "Xwm70gOdM-t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42).fit(X, y)\n",
        "\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43).fit(X, y2)\n",
        "\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44).fit(X, y3)\n",
        "\n",
        "X_new = np.array([[-0.4], [0.], [0.5]])\n",
        "print('tree.predict(X_new):\\n {}'.format(sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAeEjLLqNTJJ",
        "outputId": "e7cc8751-bd71-4e0e-dbd7-4b4807a65631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree.predict(X_new):\n",
            " [0.49484029 0.04021166 0.75026781]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(\n",
        "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
        "    n_iter_no_change=10, random_state=42).fit(X, y)\n",
        "\n",
        "print('gbrt_best.n_estimators_: {}'.format(gbrt_best.n_estimators_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEpLNNDzNoEB",
        "outputId": "9c611da9-cc52-4275-caa6-1260018c883b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gbrt_best.n_estimators_: 92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.5.3 히스토그램 기반 그레이디언트 부스팅**"
      ],
      "metadata": {
        "id": "KlrDN5n4OOtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**히스토그램 기반 그레이디언트 부스팅**\n",
        "```\n",
        "히스토그램 기반 그레이디언트 부스팅은 입력 특성을 구간으로 나누어 정수로 대체하는 방식으로 작동합니다.\n",
        "```\n",
        "**GradientBoostingClassifier와의 차이점**\n",
        "* 인스턴스 수가 10,000개보다 많으면 조기 종료가 자동으로 활성화됩니다. early_stopping 매개변수를 True 또는 False로 설정하여 조기 종료를 항상 켜거나 끌 수 있습니다.\n",
        "* subsample 매개변수가 지원되지 않습니다.\n",
        "* n_estimators 매개변수가 max_iter로 바뀌었습니다.\n",
        "* 조정할 수 있는 결정 트리 하이퍼파라미터는 max_leaf_nodes, min_samples_leaf, max_depth뿐입니다."
      ],
      "metadata": {
        "id": "_m7uRUTuPZWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "hgb_reg = make_pipeline(OrdinalEncoder(),\n",
        "                        HistGradientBoostingRegressor(categorical_features=[0], random_state=42)).fit(X, y)"
      ],
      "metadata": {
        "id": "CO_mBj4WQHXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.6 스태킹**"
      ],
      "metadata": {
        "id": "ygRbrIOqQnIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스태킹**\n",
        "```\n",
        "스태킹은 '앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시킬 수 없을까?'라는 아이디어에서 출발합니다.\n",
        "마지막 예측기를 블렌더 또는 메타 학습기라고 합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "3HYxo-MrSB8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    final_estimator = RandomForestClassifier(random_state=43),\n",
        "    cv=5\n",
        ").fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Rqxky_eYRasO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}