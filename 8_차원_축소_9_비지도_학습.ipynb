{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gf7rnklPMWp2",
        "YNlqEB163Es7",
        "Z6xahdy34iNG",
        "m1rUzHkQzb5K",
        "1gJfLgZylplQ",
        "fIZ9Csj31LF5"
      ],
      "authorship_tag": "ABX9TyO0Ga7bh0qKqRWuwTxWu8f3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiseongjun/Hands-On-Machine-Learning/blob/main/8_%EC%B0%A8%EC%9B%90_%EC%B6%95%EC%86%8C_9_%EB%B9%84%EC%A7%80%EB%8F%84_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. 차원 축소**"
      ],
      "metadata": {
        "id": "BDV-8HBa5taz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**차원의 저주**\n",
        "```\n",
        "많은 경우 머신러닝 문제는 훈련 샘플 각각이 수천 심지어 수백만 개의 특성을 가지고 있습니다.\n",
        "이런 많은 특성은 훈련을 느리게 할 뿐만 아니라 좋은 솔루션을 찾기 어렵게 만듭니다.\n",
        "이런 문제를 종종 차원의 저주라고 합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "I0acpNjc6dYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.1 차원의 저주**"
      ],
      "metadata": {
        "id": "d3GEG4uf66go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**고차원 공간**\n",
        "```\n",
        "고차원은 많은 공간을 가지고 있기 때문에 훈련 데이터가 서로 멀리 떨어져 있습니다.\n",
        "이는 새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다는 뜻입니다.\n",
        "이 경우 예측을 위해 훨씬 많은 외삽을 해야 하기 때문에 저차원일 때보다 예측이 더 불안정합니다.\n",
        "간단히 말해 훈련 세트의 차원이 클수록 과대적합 위험이 커집니다.\n",
        "```"
      ],
      "metadata": {
        "id": "CT42-qTi69Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.2 차원 축소를 위한 접근법**"
      ],
      "metadata": {
        "id": "MM196F8F9w5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.2.1 투영**"
      ],
      "metadata": {
        "id": "uUlYCc6X90rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**부분 공간과 투영**\n",
        "```\n",
        "다른 특성들은 서로 강하게 연관되어 있습니다.\n",
        "결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 놓여 있습니다.\n",
        "부분 공간에 수직으로 투영하면 저차원 데이터셋을 얻을 수 있니다.\n",
        "```\n",
        "**스위스 롤**\n",
        "```\n",
        "일부 데이터셋은 부분 공간이 뒤틀리거나 휘어 있기도 합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "bYQMZ6U4_mDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.2.2 매니폴드 학습**"
      ],
      "metadata": {
        "id": "YL9FRO7UAeP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**매니폴드 학습**\n",
        "```\n",
        "많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동합니다.\n",
        "이를 매니폴드 학습이라고 합니다.\n",
        "이는 대부분 실제 고차원 데이터셋이나 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정 또는 매니폴드 가설에 근거합니다.\n",
        "매니폴드 가정은 종종 암묵적으로 다른 가정과 병행되곤 합니다.\n",
        "이는 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이란 가정입니다.\n",
        "요약하면 모델을 훈련시키기 전에 훈련 세트의 차원을 감소시키면 훈련 속도는 빨라지지만 항상 더 낫거나 간단한 솔루션이 되는 것은 아닙니다.\n",
        "```"
      ],
      "metadata": {
        "id": "2XlsYdvhA1n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.3 주성분 분석**"
      ],
      "metadata": {
        "id": "fOxwpsXTCjHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**주성분 분석**\n",
        "```\n",
        "먼저 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 이 평면에 투영시킵니다.\n",
        "```"
      ],
      "metadata": {
        "id": "0EJ0XjEYDfe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.1 분산 보존**"
      ],
      "metadata": {
        "id": "LynG73kgDAk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**분산 보존**\n",
        "```\n",
        "저차원의 초평면에 훈련 세트를 투영하기 전에 먼저 올바른 초평면을 선택해야 합니다.\n",
        "다른 방향으로 투영하는 것보다 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적으로 보입니다.\n",
        "이 선택을 다른 방식으로 설명하면 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "jvTcwSkeDd-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.2 주성분**"
      ],
      "metadata": {
        "id": "GdNa15DGEg3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**\n",
        "```\n",
        "PCA는 훈련 세트에서 분산이 최대인 축을 찾습니다.\n",
        "또한 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾습니다.\n",
        "고차원 데이터셋이라면 PCA는 이전 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 네 번째, 다섯 번째, ..., n 번째 축을 찾습니다.\n",
        "i번째 축을 이 데이터의 i번째 주성분이라고 부릅니다.\n",
        "\n",
        "+ CAUTION\n",
        "PCA는 데이터셋의 평균을 0이라고 가정합니다.\n",
        "PCA를 직접 구현하거나 다른 라이브러리를 사용한다면 먼저 데이터를 원점에 맞추는 것을 잊어서는 안 됩니다.\n",
        "```"
      ],
      "metadata": {
        "id": "NepH8xO8FFRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9]])\n",
        "\n",
        "X_centered = X - X.mean(axis=0)\n",
        "u, s, vt = np.linalg.svd(X_centered)\n",
        "c1 = vt[0]\n",
        "c2 = vt[1]\n",
        "print('c1: {}'.format(c1))\n",
        "print('c2: {}'.format(c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQHbfvoUK5Qs",
        "outputId": "48c2277d-f255-407c-c7b8-4b564e9423fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1: [0.57735027 0.57735027 0.57735027]\n",
            "c2: [-0.81649658  0.40824829  0.40824829]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.3 d차원으로 투영하기**"
      ],
      "metadata": {
        "id": "zwbcCs6VIljE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d차원 투영**\n",
        "```\n",
        "주성분을 모두 추출해냈다면 처음 d개의 주성분을로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있습니다.\n",
        "이 초평면은 분산을 가능한 한 최대로 보존하는 투영임을 보장합니다.\n",
        "```\n",
        "**[식 8-2] 훈련 세트를 d차원으로 투영하기**\n",
        "$$X_{d-proj} = XW_{d}$$"
      ],
      "metadata": {
        "id": "n0IdT490JAck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = vt[:2].T\n",
        "X2D = X_centered @ W2\n",
        "print('X2D:\\n {}'.format(X2D))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nniDz5JELK0m",
        "outputId": "f5ee9f50-6cde-494a-ad35-e91521457a72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X2D:\n",
            " [[-5.19615242e+00 -1.33226763e-15]\n",
            " [ 0.00000000e+00  0.00000000e+00]\n",
            " [ 5.19615242e+00  1.33226763e-15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.4 사이킷런 사용하기**"
      ],
      "metadata": {
        "id": "0zsoRl1ZJtl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)\n",
        "print('X2D:\\n {}'.format(X2D))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBBsm2SmKec_",
        "outputId": "bf07d33a-37e7-44c6-b963-49c8009932b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X2D:\n",
            " [[-5.19615242e+00 -2.56395025e-16]\n",
            " [ 0.00000000e+00 -0.00000000e+00]\n",
            " [ 5.19615242e+00 -2.56395025e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.5 설명된 분산의 비율**"
      ],
      "metadata": {
        "id": "E385wWIJLmSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**설명된 분산의 비율**\n",
        "```\n",
        "explained_variance_ratio_ 변수에 저장된 주성분의 설명된 분산의 비율도 유용한 정보입니다.\n",
        "이 비율은 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타냅니다.\n",
        "```"
      ],
      "metadata": {
        "id": "c6OAe4TGLqBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk1JdMnHMBJw",
        "outputId": "9c90e5ee-ad54-4295-b7fa-dfea19c41047"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00000000e+00, 2.43475588e-33])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.6 적절한 차원 수 선택**"
      ],
      "metadata": {
        "id": "cKWK5Md9MDwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**적절한 차원 수 선택**\n",
        "```\n",
        "축소할 차원 수를 임의로 정하기보다는 충분한 분산(예: 95%)이 될 때까지 더해야 할 차원 수를 선택하는 것이 간단합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "mz22sVNmMcnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', as_frame=False)\n",
        "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
        "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWOkS6knNHMT",
        "outputId": "bbacf324-4f01-4f19-a3eb-8e46dbdfa8ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "print('pca.n_components_: {}'.format(pca.n_components_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu1e8a9ZNiaU",
        "outputId": "7d48ed78-81bb-48f5-e114-6e77c09fbcda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pca.n_components_: 154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**차원을 선택하는 다른 방법**\n",
        "```\n",
        "또 다른 방법은 설명된 분산을 차원 수에 대한 함수로 그리는 것입니다.\n",
        "일반적으로 이 그래프에는 설명된 분산의 빠른 성장이 멈추는 변곡점이 있습니다.\n",
        "마지막으로 지도 학습 작업의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝할 수 있습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "OiEMBSd3OKPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "clf = make_pipeline(PCA(random_state=42),\n",
        "                    RandomForestClassifier(random_state=42))\n",
        "param_distrib = {\n",
        "    'pca__n_components': np.arange(10, 80),\n",
        "    'randomforestclassifier__n_estimators': np.arange(50, 500)\n",
        "}\n",
        "rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\n",
        "                                random_state=42).fit(X_train[:1000], y_train[:1000])\n",
        "print(rnd_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Y6hBUpPjxT",
        "outputId": "afc893a9-c738-4552-bdb7-f858def901b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'randomforestclassifier__n_estimators': 475, 'pca__n_components': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.7 압축을 위한 PCA**"
      ],
      "metadata": {
        "id": "72-FF3iYRCI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**재구성 오차**\n",
        "```\n",
        "원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리를 재구성 오차라고 합니다.\n",
        "```\n",
        "**[식 8-3] 원본의 차원 수로 되돌리는 PCA 역변환**\n",
        "$$X_{recovered} = X_{d-proj}W^{T}_{d}$$"
      ],
      "metadata": {
        "id": "TtC9ZLFdRqZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "metadata": {
        "id": "oUyYC2RdSRF-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.8 랜덤 PCA**"
      ],
      "metadata": {
        "id": "a3PHdC8JejGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**랜덤 PCA**\n",
        "```\n",
        "svd_solver 매개변수를 'randomized'로 지정하면 사이킷런은 랜덤 PCA라 부르는 확률적 알고리즘을 사용해 처음 d개의 주성분에 대한 근삿값을 찾습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "DytGxK1wensz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_pca = PCA(n_components=154, svd_solver='randomized', random_state=42)\n",
        "X_reduced = rnd_pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "5hhlFfvkfx2G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3.9 점진적 PCA**"
      ],
      "metadata": {
        "id": "0t-FknZPgycj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**점진적 PCA(IPCA)**\n",
        "```\n",
        "PCA 구현의 문제는 SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 한다는 것입니다.\n",
        "훈련 세트를 미니배치로 나눈 뒤 IPCA 알고리즘에 한 번에 하나씩 주입합니다.\n",
        "이런 방식은 훈련 세트가 클 때 유용하고 온라인으로 PCA를 적용할 수도 있습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "WOo3WyQng8Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X_train, n_batches):\n",
        "  inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced = inc_pca.transform(X_train)\n",
        "\n",
        "filename = 'my_mnist.mmap'\n",
        "X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\n",
        "X_mmap[:] = X_train\n",
        "X_mmap.flush()\n",
        "\n",
        "X_mmap = np.memmap(filename, dtype='float32', mode='readonly').reshape(-1, 784)\n",
        "batch_size = X_mmap.shape[0] // n_batches\n",
        "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size).fit(X_mmap)"
      ],
      "metadata": {
        "id": "DpB5DBNDjOro"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.4 랜덤 투영**"
      ],
      "metadata": {
        "id": "_e99Y1PDkYkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**랜덤 투영**\n",
        "```\n",
        "랜덤 투영 알고리즘은 랜덤한 선형 투영을 사용하여 데이터를 저차원 공간에 투영합니다.\n",
        "랜덤한 투영은 실제로 거리를 상당히 잘 보존할 가능성이 매우 높다는 것이 밝혀졌습니다.\n",
        "따라서 투영 후에도 비슷한 두 개의 샘플은 비슷한 채로 남고 매우 다른 두 개의 샘플은 매우 다른 채로 남습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "-7pFhvlLpYNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "\n",
        "m, e = 5_000, 0.1\n",
        "d = johnson_lindenstrauss_min_dim(m, eps=e)\n",
        "\n",
        "n = 20_000\n",
        "np.random.seed(42)\n",
        "P = np.random.randn(d, n) / np.sqrt(d)\n",
        "\n",
        "X = np.random.randn(m, n)\n",
        "X_reduced = X @ P.T"
      ],
      "metadata": {
        "id": "E1KGDb3Lsl3u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "gaussian_rnd_proj = GaussianRandomProjection(eps=e, random_state=42)\n",
        "X_reduced = gaussian_rnd_proj.fit_transform(X)"
      ],
      "metadata": {
        "id": "afEewaxJsuZt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.5 지역 선형 임베딩**"
      ],
      "metadata": {
        "id": "_zBAagQltO9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**지역 선형 임베딩(비선형 차원 축소 기술)**\n",
        "```\n",
        "LLE는 먼저 각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정합니다.\n",
        "그런 다음 국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾습니다.\n",
        "이 방법은 특히 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는 데 좋습니다.\n",
        "```\n",
        "**[식 8-4] LLE 단계 1: 선형적인 지역 관계 모델링**\n",
        "$$\\hat{W} = argmin_{w}\\sum^{m}_{i=1}(X^{(i)}-\\sum^{m}_{j=1}w_{i, j}X^{(j)})^2$$\n",
        "* $w_{i, j} = 0$ $X^{(j)}$가 $X^{(i)}$의 최근접 이웃 k개 중 하나가 아닐 때\n",
        "* $\\sum^{m}_{j=1}w_{i, j} = 1$ $i = 1, 2, …, m$일 때\n",
        "\n",
        "**[식 8-5] LLE 단계 2: 관계를 보존하는 차원 축소**\n",
        "$$\\hat{Z} = argmin\\sum^{m}_{i=1}(z^{(i)}-\\sum^{m}_{j=1}\\hat{w}_{i, j}z^{(j)})$$"
      ],
      "metadata": {
        "id": "b_y7U0DftyPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
        "X_unrolled = lle.fit_transform(X_swiss)"
      ],
      "metadata": {
        "id": "FOcfWSMguM9D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.6 다른 차원 축소 기법**"
      ],
      "metadata": {
        "id": "QmRhr3tqwe3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**다차원 스케일링**\n",
        "```\n",
        "다차원 스케일링은 샘플 간의 거리를 보존하면서 차원을 축소합니다.\n",
        "랜덤 투영은 고차원 데이터에는 적합하지만 저차원 데이터에는 잘 작동하지 않습니다.\n",
        "```\n",
        "**Isomap**\n",
        "```\n",
        "Isomap은 각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만듭니다.\n",
        "그런 다음 샘플 간의 지오데식 거리를 유지하면서 차원을 축소합니다.\n",
        "그래프에서 두 노드 사이의 지오데식 거리는 두 노드 사이의 최단 경로를 이루는 노드의 수입니다.\n",
        "```\n",
        "**t-SNE**\n",
        "```\n",
        "t-SNE는 비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소합니다.\n",
        "주로 시각화에 많이 사용되며 특히 고차원 공간에 있는 샘플의 군집을 시각화할 때 사용됩니다.\n",
        "```\n",
        "**선형 판별 분석**\n",
        "```\n",
        "선형 판별 분석은 선형 분류 알고리즘입니다.\n",
        "하지만 클래스 사이를 가장 잘 구분하는 축을 학습합니다.\n",
        "이 축은 데이터가 투영되는 초평면을 정의하는 데 사용될 수 있습니다.\n",
        "이 알고리즘의 장점은 투영을 통해 가능한 한 클래스를 멀리 떨어지게 유지시키므로 다른 분류 알고리즘을 적용하기 전에 차원을 축소시키는 데 좋습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "HYZRPexpwhth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 비지도 학습**"
      ],
      "metadata": {
        "id": "v-GRfzxIxoTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**군집**\n",
        "```\n",
        "비슷한 샘플을 클러스터로 모읍니다.\n",
        "군집은 데이터 분석, 고객 분류, 추천 시스템, 검색 엔진 등에 사용할 수 있는 훌륭한 도구입니다.\n",
        "```\n",
        "**이상치 탐지**\n",
        "```\n",
        "'정상' 데이터가 어떻게 보이는지 학습합니다.\n",
        "그다음 비정상 샘플을 감지하는 데 사용합니다.\n",
        "이러한 샘플을 이상치라고 하고 정상 샘플을 정상치라고 합니다.\n",
        "```\n",
        "**밀도 추정**\n",
        "```\n",
        "데이터셋 생성 확률 과정의 확률 밀도 함수를 추정합니다.\n",
        "밀도 추정은 이상치 탐지에 널리 사용됩니다.\n",
        "밀도가 매우 낮은 영역에 놓인 샘플이 이상치일 가능성이 높습니다.\n",
        "또한 데이터 분석과 시각화에도 유용합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "-JSqt5axxx1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.1 군집**"
      ],
      "metadata": {
        "id": "Ho-rKvyNKCeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**군집**\n",
        "```\n",
        "군집은 비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "unxHEKbrMNX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.1 k-평균**"
      ],
      "metadata": {
        "id": "gf7rnklPMWp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-평균**\n",
        "```\n",
        "k-평균은 반복 몇 번으로 이런 종류의 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "hThurdTxMm1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(n_samples=2000, random_state=42)\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "print('y_pred:\\n {}'.format(y_pred))\n",
        "print('\\ny_pred is kmeans.labels_: {}'.format(y_pred is kmeans.labels_))\n",
        "print('\\nkmeans.cluster_centers_:\\n {}'.format(kmeans.cluster_centers_))\n",
        "\n",
        "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
        "print('\\nkmeans.predict(X_new):\\n {}'.format(kmeans.predict(X_new)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrQ6mzpONxIL",
        "outputId": "4424d35c-6d84-48ef-a2b1-414ad39ce67c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred:\n",
            " [2 4 2 ... 4 1 4]\n",
            "\n",
            "y_pred is kmeans.labels_: True\n",
            "\n",
            "kmeans.cluster_centers_:\n",
            " [[-1.98634961  8.4238017 ]\n",
            " [-6.88074384 -6.90211374]\n",
            " [ 4.46534689  1.23916602]\n",
            " [-2.97468946  9.69770001]\n",
            " [ 4.78857951  2.78554639]]\n",
            "\n",
            "kmeans.predict(X_new):\n",
            " [2 2 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**하드 군집과 소프트 군집**\n",
        "```\n",
        "샘플을 하나의 클러스터에 할당하는 하드 군집보다 클러스터 마다 샘플에 점수를 부여하는 소프트 군집이 유용할 수 있습니다.\n",
        "KMeans 클래스의 transform() 메서드는 샘플과 각 센트로이드 사이의 거리를 반환합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "b-NVOoNgPvVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.transform(X_new).round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN04R60KQ90K",
        "outputId": "3a23457c-cec1-410a-ab0f-4a65de157b79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.72, 11.25,  4.53,  8.25,  4.85],\n",
              "       [ 8.13, 13.3 ,  1.65,  9.74,  1.95],\n",
              "       [ 5.52, 10.64,  7.67,  6.7 ,  7.79],\n",
              "       [ 6.01, 10.17,  7.57,  7.2 ,  7.79]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-평균 알고리즘**\n",
        "```\n",
        "처음에는 센트로이드를 랜덤하게 선정합니다.\n",
        "그다음 샘플에 레이블을 할당하고 센트로이드를 업데이트하고,\n",
        "샘플에 레이블을 할당하고 센트로이드를 업데이트하는 식으로 센트로이드에 변화가 없을 때까지 계속합니다.\n",
        "샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리는 각 단계마다 내려갈 수만 있고 음수가 될 수 없기 때문에 수렴이 보장됩니다.\n",
        "```\n",
        "**센트로이드 초기화 방법**\n",
        "```\n",
        "센트로이드를 초기화하는 방법은 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택하는 것입니다.\n",
        "랜덤 초기화 횟수는 n_init 매개변수로 조절합니다. 기본값은 10입니다.\n",
        "모델의 이너셔는 각 샘플과 가장 가까운 센트로이드 사이의 제곱 거리 합으로, 사용하는 성능 지표입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "UxHYRhbtRB2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
        "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42).fit(X)\n",
        "print('kmeans.inertia_: {}'.format(kmeans.inertia_))\n",
        "print('kmeans.score: {}'.format(kmeans.score(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9IsfmfPmdw-",
        "outputId": "a34b4039-0ecd-41ff-b796-9a00cf4babce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kmeans.inertia_: 3213.992161105195\n",
            "kmeans.score: -3213.9921611051946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-평균 속도 개선**\n",
        "```\n",
        "클러스터가 많은 일부 대규모 데이터셋에서 불필요한 거리 계산을 피함으로써 알고리즘의 속도를 상당히 높일 수 있다는 것입니다.\n",
        "엘칸은 이를 위해 삼각 부등식을 사용했습니다.\n",
        "샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지합니다.\n",
        "그러나 엘칸의 알고리즘이 항상 훈련 속도를 높일 수 있는 것은 아니며 때로는 훈련 속도가 상당히 느려질 수도 있는데, 이는 데이터셋에 따라 다릅니다.\n",
        "```\n",
        "**미니배치 k-평균**\n",
        "```\n",
        "전체 데이터셋을 사용해 반복하지 않고 각 반복마다 미니배치 사용해 센트로이드를 조금씩 이동합니다.\n",
        "이는 일반적으로 알고리즘의 속도를 높입니다.\n",
        "또한 메모리에 들어가지 않은 대량의 데이터셋에 군집 알고리즘을 적용할 수 있습니다.\n",
        "미니배치 k-평균 알고리즘이 일반 k-평균 알고리즘보다 훨씬 빠르지만 이너셔는 일반적으로 조금 더 나쁩니다.\n",
        "```"
      ],
      "metadata": {
        "id": "mW-N49x8osUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42).fit(X)"
      ],
      "metadata": {
        "id": "VdJuMgKwqRMe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**최적의 클러스터 개수 찾기**\n",
        "```\n",
        "최선의 클러스터 개수를 선택하는 방법은 실루엣 점수입니다.\n",
        "이 값은 모든 샘플에 대한 실루엣 계수의 평균입니다.\n",
        "샘플의 실루엣 계수는 (b-a)/max(a, b)로 계산됩니다.\n",
        "a는 동일한 클러스터에 있는 다른 샘플까지 평균 거리입니다.\n",
        "b는 가장 가까운 클러스터까지 평균 거리입니다.\n",
        "실루엣 계수는 -1에서 +1까지 바뀔 수 있습니다.\n",
        "+1에 가까우면 자신의 클러스터 안에 잘 속해 있고 다른 클러스터와는 멀리 떨어져 있다는 뜻입니다.\n",
        "실루엣 계수가 0에 가까우면 클러스터 경계에 위치한다는 의미이고 -1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "5eq93cvO0opg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print('silhouette_score: {}'.format(silhouette_score(X, kmeans.labels_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JAQf4Um24IK",
        "outputId": "8890b8ad-80aa-4713-bede-67df913b0d01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "silhouette_score: 0.6603376174628922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.2 k-평균의 한계**"
      ],
      "metadata": {
        "id": "YNlqEB163Es7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-평균의 한계**\n",
        "```\n",
        "k-평균이 완벽한 것은 아닙니다.\n",
        "최적이 아닌 솔루션을 피하려면 알고리즘을 여러 번 실행해야 합니다.\n",
        "또한 클러스터 개수를 지정해야 합니다.\n",
        "그리고 k-평균은 클러스터의 크기 또는 밀집도가 서로 다르거나 원형이 아닐 경우 잘 작동하지 않습니다.\n",
        "k-평균을 실행하기 전에 입력 특성의 스케일을 맞추는 것이 중요합니다.\n",
        "그렇지 않으면 클러스터가 길쭉해지고 k-평균의 결과가 좋지 않습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "VDNfJ-dw3ncZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.3 군집을 사용한 이미지 분할**"
      ],
      "metadata": {
        "id": "Z6xahdy34iNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**이미지 분할**\n",
        "```\n",
        "이미지 분할은 이미지를 여러 개의 세그먼트로 분할하는 작업입니다.\n",
        "```\n",
        "* **색상 분할**\n",
        "  * 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당합니다.\n",
        "* **시맨틱 분할**\n",
        "  * 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당합니다.\n",
        "* **인스턴스 분할**\n",
        "  * 개별 객체에 속한 모든 픽셀을 같은 세그먼트에 할당합니다."
      ],
      "metadata": {
        "id": "V6mPfxkM5LO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.4 군집을 사용한 준지도 학습**"
      ],
      "metadata": {
        "id": "w3aOUXdA50w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**준지도 학습**\n",
        "```\n",
        "군집을 사용하는 또 다른 사례는 준지도 학습입니다. 레이블이 없는 데이터가 많고 레이블이 있는 데이터는 적을 때 사용합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "Id3-Qlsa6jpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_digits, y_digits = load_digits(return_X_y=True)\n",
        "X_train, y_train = X_digits[:1400], y_digits[:1400]\n",
        "X_test, y_test = X_digits[1400:], y_digits[1400:]\n",
        "\n",
        "n_labeled = 50\n",
        "log_reg = LogisticRegression(max_iter=10_000).fit(X_train[:n_labeled], y_train[:n_labeled])\n",
        "print('log_reg.score: {}'.format(log_reg.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1B0JvYB7N6D",
        "outputId": "86b4a0ed-e7bc-489d-9c62-9a698ee2e165"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_reg.score: 0.7581863979848866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 50\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "X_digits_dist = kmeans.fit_transform(X_train)\n",
        "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
        "X_representative_digits = X_train[representative_digit_idx]\n",
        "y_representative_digits = np.array([\n",
        "    8, 0, 1, 3, 6, 7, 5, 4, 2, 8,\n",
        "    2, 3, 9, 5, 3, 9, 1, 7, 9, 1,\n",
        "    4, 6, 9, 7, 5, 2, 2, 1, 3, 3,\n",
        "    6, 0, 4, 9, 8, 1, 8, 4, 2, 4,\n",
        "    2, 3, 9, 7, 8, 9, 6, 5, 6, 4\n",
        "])\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=10_000).fit(X_representative_digits, y_representative_digits)\n",
        "print('log_reg.score: {}'.format(log_reg.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAG5dEu98jtn",
        "outputId": "2c8f0960-4dd6-412f-d8ab-a0868d32de1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_reg.score: 0.8312342569269522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_propagated = np.empty(len(X_train), dtype=np.int64)\n",
        "for i in range(k):\n",
        "  y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n",
        "\n",
        "log_reg = LogisticRegression().fit(X_train, y_train_propagated)\n",
        "print('log_reg.score: {}'.format(log_reg.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH8jTjWM-DBv",
        "outputId": "49b1883c-cdb7-42ce-db3d-966b68d95b22"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_reg.score: 0.8664987405541562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentile_closest = 99\n",
        "\n",
        "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
        "for i in range(k):\n",
        "  in_cluster = (kmeans.labels_ == i)\n",
        "  cluster_dist = X_cluster_dist[in_cluster]\n",
        "  cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
        "  above_cutoff = (X_cluster_dist > cutoff_distance)\n",
        "  X_cluster_dist[in_cluster & above_cutoff] = -1\n",
        "\n",
        "partially_propagated = (X_cluster_dist != -1)\n",
        "X_train_partially_propagated = X_train[partially_propagated]\n",
        "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=10_000).fit(X_train_partially_propagated, y_train_partially_propagated)\n",
        "print('log_reg.score: {}'.format(log_reg.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDQfME4rDtu",
        "outputId": "d24ca372-db77-417e-e4ca-c7e5e729bc0a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_reg.score: 0.8614609571788413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.5 DBSCAN**"
      ],
      "metadata": {
        "id": "m1rUzHkQzb5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DBSCAN**\n",
        "```\n",
        "DBSCAN 알고리즘은 밀집된 연속적 지역을 클러스터로 정의합니다.\n",
        "작동 방식은 다음과 같습니다.\n",
        "```\n",
        "* 알고리즘이 각 샘플에서 작은 거리인 $\\epsilon$ 내에 샘플이 몇 개 놓여 있는지 셉니다. 이 지역을 샘플의 $\\epsilon$-이웃이라고 부릅니다.\n",
        "* $\\epsilon$-이웃 내에 적어도 min_samples개 샘플이 있다면 이를 핵심 샘플로 간주합니다. 즉, 핵심 샘플은 밀집된 지역에 있는 샘플입니다.\n",
        "* 핵심 샘플의 이웃에 있는 모든 샘플은 동일한 클러스터에 속합니다. 이웃에는 다른 핵심 샘플이 포함될 수 있습니다. 따라서 핵심 샘플의 이웃의 이웃은 계속해서 하나의 클러스터를 형성합니다.\n",
        "* 핵심 샘플이 아니고 이웃도 아닌 샘플은 이상치로 판단합니다.\n"
      ],
      "metadata": {
        "id": "VBodUKh40MtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.05)\n",
        "dbscan = DBSCAN(eps=0.05, min_samples=5).fit(X)\n",
        "print('dbscan.labels_: {}'.format(dbscan.labels_[:5]))\n",
        "print('\\ndbscan.core_sample_indices_: {}'.format(dbscan.core_sample_indices_[:5]))\n",
        "print('\\ndbscan.components_: \\n{}'.format(dbscan.components_[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFpQkCjXoM5j",
        "outputId": "5eee3004-45ca-42ec-a51d-4c98a55873ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dbscan.labels_: [ 0  1  2 -1  1]\n",
            "\n",
            "dbscan.core_sample_indices_: [0 1 2 4 5]\n",
            "\n",
            "dbscan.components_: \n",
            "[[-0.99798925  0.23770659]\n",
            " [ 0.98654074  0.23788903]\n",
            " [ 1.88685205  0.02653525]\n",
            " [ 1.01675881  0.17760595]\n",
            " [ 2.03971642  0.35384904]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=50).fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
        "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
        "print('knn.predict: \\n{}'.format(knn.predict(X_new)))\n",
        "print('\\nknn.predict_proba: \\n{}'.format(knn.predict_proba(X_new)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byL_COHEpXCG",
        "outputId": "5a864ff6-1e53-4df7-8a48-7a8b67e9d52a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knn.predict: \n",
            "[0 5 1 2]\n",
            "\n",
            "knn.predict_proba: \n",
            "[[0.7  0.   0.   0.   0.14 0.16 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.02 0.22 0.52 0.   0.24 0.  ]\n",
            " [0.   0.48 0.   0.   0.36 0.   0.16 0.   0.  ]\n",
            " [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1.6 다른 군집 알고리즘**"
      ],
      "metadata": {
        "id": "1gJfLgZylplQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**사이킷런에는 살펴볼 만한 여러 군집 알고리즘이 구현되어 있습니다.**\n",
        "* **병합 군집**\n",
        "* **BIRCH**\n",
        "* **평균-이동**\n",
        "* **유사도 전파**\n",
        "* **스펙트럼 군집**"
      ],
      "metadata": {
        "id": "V3FK4Mgzlsk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.2 가우스 혼합**"
      ],
      "metadata": {
        "id": "fIZ9Csj31LF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가우스 혼합**\n",
        "```\n",
        "가우스 혼합 모델은 샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델입니다.\n",
        "하나의 가우스 분포에서 생성된 모든 샘플은 하나의 클러스터를 형성합니다.\n",
        "일반적으로 이 클러스터는 타원형입니다.\n",
        "샘플이 주어지면 가우스 분포 중 하나에서 생성되었다는 것은 알지만 어떤 분포인지 또 이 분포의 파라미터는 무엇인지 알지 못합니다.\n",
        "```\n",
        "**GaussianMixture 클래스**\n",
        "```\n",
        "사전에 가우스 분포의 개수 k를 알아야 합니다.\n",
        "데이터셋 X가 다음 확률 과정을 통해 생성되었다고 가정합니다.\n",
        "```\n",
        "* 샘플마다 $k$개의 클러스터에서 랜덤하게 한 클러스터가 선택됩니다. $j$번째 클러스터를 선택할 확률은 클러스터의 가중치 $\\phi^{(j)}$입니다. $i$번째 샘플을 위해 선택한 클러스터 인덱스는 $z^{(i)}$로 표시됩니다.\n",
        "* $i$번째 샘플이 $j$번째 클러스터에 할당되었다면$(z^{(i)}=j)$이 샘플의 위치 $x^{(i)}$는 평균이 $\\mu^{(j)}$이고 공분산 행렬이 $\\sum^{(j)}$인 가우스 분포에서 랜덤하게 샘플링됩니다. 이를 $x^{(i)}$ ~ $𝒩(\\mu^{(j)}, \\sum^{(j)})$와 같이 씁니다."
      ],
      "metadata": {
        "id": "FczsxBhC1_tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm = GaussianMixture(n_components=3, n_init=10, random_state=42).fit(X)\n",
        "print('gm.weights_: \\n{}'.format(gm.weights_))\n",
        "print('\\ngm.means_: \\n{}'.format(gm.means_))\n",
        "print('\\ngm.covariances_: \\n{}'.format(gm.covariances_))\n",
        "\n",
        "print('\\ngm.converged_: {}'.format(gm.converged_))\n",
        "print('\\ngm.n_iter_: {}'.format(gm.n_iter_))\n",
        "print('\\ngm.predict: \\n{}'.format(gm.predict(X)))\n",
        "print('\\ngm.predict_proba: \\n{}'.format(gm.predict_proba(X).round(3)))\n",
        "\n",
        "X_new, y_new = gm.sample(6)\n",
        "print('\\nX_new: \\n{}'.format(X_new))\n",
        "print('\\ny_new: {}'.format(y_new))\n",
        "print('\\ngm.score_smaples: {}'.format(gm.score_samples(X).round(2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd-q_fZ63ICy",
        "outputId": "09ebd5e0-02a7-4f95-e927-c8464ad6effb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gm.weights_: \n",
            "[0.59168733 0.20153089 0.20678177]\n",
            "\n",
            "gm.means_: \n",
            "[[ 0.49248283  0.25581333]\n",
            " [-0.7545737   0.55520819]\n",
            " [ 1.73222992 -0.06668853]]\n",
            "\n",
            "gm.covariances_: \n",
            "[[[ 0.16818325 -0.10138525]\n",
            "  [-0.10138525  0.28754971]]\n",
            "\n",
            " [[ 0.05015     0.05923078]\n",
            "  [ 0.05923078  0.08398829]]\n",
            "\n",
            " [[ 0.05556608  0.06385008]\n",
            "  [ 0.06385008  0.08804306]]]\n",
            "\n",
            "gm.converged_: True\n",
            "\n",
            "gm.n_iter_: 15\n",
            "\n",
            "gm.predict: \n",
            "[1 0 2 0 0 2 0 1 1 1 1 0 2 2 0 0 0 1 2 0 0 0 1 0 0 2 1 2 0 0 0 0 0 0 0 1 0\n",
            " 1 2 0 2 2 0 0 1 1 1 0 0 2 1 0 1 2 0 1 0 0 1 0 2 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 2 0 1 0 2 0 2 1 2 2 2 0 0 1 1 1 0 1 2 2 2 0 1 0 0 0 0 2 0 0 0 2 0 1 0\n",
            " 1 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 2 0 1 0 0 0 0 0 0 1 0 0 1 2 0 0 0 0 0 0 0 1 0 0 0 1 0 0 2 0 2 0 1 0 0 0\n",
            " 0 2 0 0 2 1 0 1 0 0 0 2 1 1 0 1 2 0 1 2 0 0 2 1 0 2 0 0 2 0 1 0 0 2 2 0 0\n",
            " 1 0 1 0 2 2 0 0 0 2 2 0 0 1 2 0 0 2 0 2 1 0 0 2 2 1 0 0 0 1 0 0 1 0 0 1 2\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 2 0 1 1 0 0 1 0 0 0 0 0 2 2 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 0 0 1 2 0 2 0 1 0 0 2 2 0 0 1 2 0 0 0 0 2 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 2 0 0 0 2 1 2 0 0 0 0 0 2 0 1 0 0 0 0 0 0 2 0 0\n",
            " 0 0 0 2 0 0 0 0 2 1 1 2 0 0 0 1 1 2 1 0 0 1 2 0 1 2 0 2 2 0 0 1 1 0 2 2 1\n",
            " 1 0 0 1 0 0 2 1 1 0 0 1 0 0 2 2 0 2 2 0 2 0 2 1 0 2 1 0 0 0 2 0 0 1 0 0 2\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 2 2 0 2 1 0 1 1 0 1 0 2 1 0 1 2 0 1 0 2 1 2\n",
            " 2 0 0 2 0 0 1 0 0 1 2 0 0 2 0 2 2 2 0 0 0 0 2 0 1 0 0 0 0 1 0 1 0 2 2 1 2\n",
            " 0 0 1 0 0 2 2 1 0 0 0 0 0 0 1 0 0 2 0 0 2 0 0 1 2 0 0 0 0 0 1 0 1 1 0 0 2\n",
            " 0 1 2 0 0 0 1 2 0 0 0 1 0 2 0 1 0 0 0 0 1 2 1 0 2 1 0 0 0 2 2 1 1 0 0 0 1\n",
            " 2 1 0 2 1 0 0 1 0 2 0 2 0 2 1 2 0 0 0 0 0 1 1 1 2 2 0 0 0 0 0 0 0 0 0 1 1\n",
            " 2 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 2 0 0 1 0 0 0 0 0 0 0 0 0 1 1 2 0 2 1 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 2 1\n",
            " 0 2 0 0 0 0 1 0 0 0 1 2 1 0 0 0 0 0 1 2 0 2 2 0 0 0 1 2 0 0 0 0 0 2 1 2 0\n",
            " 0 0 0 2 0 0 0 1 2 2 0 1 2 0 0 0 1 1 2 2 0 2 0 0 2 0 2 0 0 0 0 2 0 2 0 1 0\n",
            " 0 0 2 0 2 0 2 1 0 2 2 1 1 0 2 1 1 0 2 0 2 0 0 2 0 0 0 0 0 1 2 1 0 0 2 2 0\n",
            " 2 0 1 0 0 2 2 2 0 1 0 0 2 2 0 0 2 0 0 0 2 0 0 0 2 0 0 0 2 2 1 2 2 1 1 0 2\n",
            " 0 0 0 1 0 0 0 1 1 2 2 2 0 0 0 0 0 1 2 0 0 0 0 1 0 2 0 2 1 0 0 0 1 2 1 2 2\n",
            " 2 1 0 0 1 2 0 0 0 2 0 1 0 0 1 1 0 0 2 2 0 1 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0\n",
            " 2 1 0 0 0 0 0 2 0 0 2 2 0 0 2 0 0 0 0 0 0 0 1 0 0 2 0 0 1 0 0 2 0 0 1 1 0\n",
            " 0 1 0 1 1 0 2 0 0 2 2 0 2 1 1 1 0 0 2 0 0 2 0 1 1 0 0 2 0 2 2 1 0 0 0 0 0\n",
            " 2]\n",
            "\n",
            "gm.predict_proba: \n",
            "[[0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " ...\n",
            " [0.56  0.44  0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.045 0.    0.955]]\n",
            "\n",
            "X_new: \n",
            "[[ 0.74247688  0.8372868 ]\n",
            " [ 0.49331547  0.09774595]\n",
            " [ 0.25293468  1.18812962]\n",
            " [ 0.78582354  0.105647  ]\n",
            " [-0.62696985  0.66717249]\n",
            " [-0.69388177  0.4007071 ]]\n",
            "\n",
            "y_new: [0 0 0 0 1 1]\n",
            "\n",
            "gm.score_smaples: [-0.43 -1.63 -0.32 -1.42 -1.67 -0.86 -1.53 -0.02 -0.82 -1.2  -0.14 -1.98\n",
            " -0.31 -0.58 -1.63 -1.86 -1.51 -0.02 -0.27 -1.62 -1.46 -1.29 -0.88 -2.16\n",
            " -1.48 -0.84 -0.05 -0.95 -1.4  -1.45 -1.54 -1.59 -1.6  -1.63 -1.73 -0.74\n",
            " -1.41 -1.19 -0.27 -1.95 -0.85 -0.45 -1.51 -1.32 -0.37 -0.75 -0.01 -1.45\n",
            " -1.54 -0.16 -1.73 -1.77 -0.98 -2.1  -2.23 -1.01 -2.1  -1.8  -0.66 -1.32\n",
            "  0.07 -0.32 -1.77 -0.6  -1.23 -0.15 -2.31 -0.23 -0.59 -1.84 -0.32 -1.43\n",
            " -1.46 -1.3  -2.12 -1.68 -0.03 -1.54  0.06 -1.71 -1.38 -1.32 -1.16 -1.27\n",
            " -0.08 -2.23 -0.37 -1.53 -1.38  0.09  0.08 -0.62 -1.72 -2.   -0.11 -0.29\n",
            " -0.69 -1.42 -0.57 -1.67 -1.7  -1.43 -1.77 -1.28 -1.42 -2.03 -2.07 -0.2\n",
            " -1.51 -1.3  -1.73 -0.48 -1.62 -1.76 -1.83 -1.48 -2.51 -1.52 -1.4  -0.01\n",
            " -1.66 -1.58 -1.73 -1.76 -1.86 -1.66 -1.69 -1.9  -1.3  -2.23 -1.6  -0.44\n",
            " -1.51 -0.48 -1.85 -2.22 -1.58 -1.83 -1.68 -1.67 -1.94 -1.9  -1.69 -1.72\n",
            " -0.64 -1.48 -1.44 -2.19 -1.84 -1.1  -1.83 -2.   -1.46 -1.7  -1.55 -1.6\n",
            " -1.73 -1.46 -0.09 -1.4  -1.55 -0.07 -0.39 -1.86 -1.39 -1.75 -1.77 -1.48\n",
            " -1.64 -1.73 -1.07 -1.48 -1.93 -1.44 -0.22 -1.42 -1.67 -0.49 -1.71 -0.21\n",
            " -1.76 -0.02 -1.71 -1.98 -1.72 -1.84 -0.85 -1.47 -1.57 -0.54  0.   -1.51\n",
            " -0.68 -1.65 -1.29 -1.97 -1.44 -0.26 -2.05 -1.47 -1.46 -0.59 -1.26 -3.02\n",
            " -0.74 -2.03 -1.68 -1.54 -0.12 -1.6  -0.5  -1.75 -1.73 -1.91 -1.68 -1.93\n",
            " -1.63 -1.76  0.05 -1.97 -1.5  -1.64 -0.83 -1.3  -0.65 -2.28 -0.58 -1.32\n",
            " -1.51 -1.55 -1.6  -0.27 -0.75 -1.57 -2.12 -1.24 -1.02 -1.65 -1.45 -1.\n",
            " -2.18 -3.79 -0.34 -1.52 -1.58 -1.63 -1.06 -1.25 -1.66 -1.35 -1.99 -1.56\n",
            " -1.64 -1.7  -0.41 -1.78 -1.82 -1.58 -0.1  -0.67 -1.54 -0.45 -1.66 -1.71\n",
            " -0.21 -1.79 -2.07  0.08 -1.38 -1.22 -1.67 -2.28 -0.71 -0.31 -1.17 -0.37\n",
            " -2.   -0.24 -0.2  -1.72 -1.53 -2.08 -1.55 -1.72 -1.67 -1.91 -1.72 -0.93\n",
            " -0.31 -1.54 -1.78 -1.51 -1.59 -1.74 -1.64 -1.95 -0.99 -1.45 -1.52 -1.79\n",
            " -1.4   0.02 -1.71 -0.4  -1.78 -1.3  -1.83 -1.52 -1.59 -0.1  -0.77 -1.64\n",
            " -1.13 -1.37 -0.57 -1.73 -1.53 -0.46 -0.17 -1.8  -1.79 -0.08 -0.33 -1.32\n",
            " -1.76 -1.72 -1.75  0.1  -1.34 -1.7  -2.06 -1.81 -1.47 -1.63 -1.52 -1.67\n",
            " -1.42 -1.5  -0.51 -1.36 -1.86 -1.79 -1.29 -1.55 -1.38 -1.34 -1.38 -2.17\n",
            " -1.8  -1.8  -2.9  -1.49 -0.02 -2.24 -1.72 -2.2  -1.5  -2.1  -0.82 -1.5\n",
            " -0.55 -2.12 -2.03 -1.49 -1.48 -2.32 -1.95 -0.48 -1.59 -1.58 -1.42 -1.88\n",
            " -1.8  -0.17 -1.31 -2.22 -1.93 -2.16 -0.18 -0.33  0.16 -0.39 -1.91 -1.73\n",
            " -1.64 -1.22 -0.28 -4.46 -0.25 -1.76 -1.37 -1.72 -0.38 -1.99 -0.23 -0.48\n",
            " -1.5  -1.1  -0.09 -1.36 -1.81  0.13 -0.44 -1.76 -0.53 -1.23 -0.79 -0.3\n",
            " -1.38 -1.8   0.12 -1.86 -1.71 -0.32 -0.43  0.18 -1.41 -1.25 -0.48 -1.81\n",
            " -2.09 -0.67 -1.39 -1.65 -1.38 -0.07 -1.58 -0.38 -1.65 -0.29  0.09 -1.33\n",
            " -0.54 -0.51 -1.65 -1.35 -1.57 -0.49 -1.43 -1.55 -0.86 -2.3  -1.46 -0.42\n",
            " -2.29 -2.11 -0.1  -0.08 -1.44 -2.17 -2.02 -1.51 -1.69 -1.59 -1.91 -2.3\n",
            " -1.67 -1.76 -1.66 -0.62 -0.36 -1.76 -0.49 -0.08 -1.45 -0.68 -1.43 -2.24\n",
            " -0.63 -1.9  -3.35 -1.78 -2.01 -0.56 -1.25 -1.58 -0.48 -1.88 -0.99 -0.72\n",
            " -0.39 -1.8  -1.56 -2.12 -0.11 -2.11 -1.45 -1.02 -1.52 -1.69 -0.48 -2.86\n",
            " -2.12 -1.71 -0.3  -2.15  0.01 -1.   -1.56 -1.17 -1.6  -1.67 -2.13  0.13\n",
            " -1.72 -2.27 -1.83 -1.37 -1.63 -1.56 -0.91 -2.22 -0.33 -1.99 -0.03  0.04\n",
            " -2.64 -0.64 -1.37 -1.68 -0.37 -1.56 -1.62 -0.39 -2.3  -0.72 -1.71 -2.09\n",
            " -1.95 -1.51 -2.19 -1.31 -1.11 -1.72 -1.7  -0.29 -1.61 -1.71 -0.6  -1.49\n",
            " -1.39 -0.67  0.02 -1.46 -1.7  -1.52 -1.51 -1.56 -1.35 -1.61 -0.53 -0.84\n",
            " -1.56 -1.39 -0.23 -1.57 -0.17 -1.24 -1.86 -2.06 -1.95 -0.63 -0.93 -1.76\n",
            " -1.94 -1.98 -1.07 -2.22  0.08 -1.76  0.13 -1.71 -1.41 -1.88 -1.35 -0.11\n",
            " -0.45 -1.11 -1.78 -1.13 -2.09 -1.45 -1.43 -1.35 -0.83 -0.59 -1.34 -0.39\n",
            " -1.56 -1.44 -2.31 -0.65 -0.73 -0.55 -1.48 -0.13 -0.35 -1.96 -1.37 -0.82\n",
            " -2.09 -1.09 -1.59 -0.46 -2.17 -1.39 -0.21  0.07 -1.47 -1.53 -1.76 -1.63\n",
            " -1.63 -0.25  0.06 -0.58 -0.17 -1.39 -1.75 -1.53 -1.55 -1.48 -1.53 -2.02\n",
            " -2.18 -1.92 -1.68 -0.69 -0.36 -0.41 -1.71 -0.14 -1.61  0.03 -1.75 -1.69\n",
            " -1.57 -0.23 -1.93 -0.14 -2.06 -1.54 -1.89 -1.45 -2.26 -1.78 -1.5  -1.74\n",
            " -1.97 -1.22 -1.78 -1.54 -0.63 -1.73  0.09 -2.17 -1.29 -1.62 -1.65 -1.75\n",
            " -2.19 -2.11 -1.37 -1.51 -1.83 -1.52 -1.51 -1.36 -1.53 -1.76 -1.6  -1.65\n",
            " -1.79 -2.05 -1.31 -1.86 -1.52 -2.   -1.32 -2.05 -0.08 -0.8  -0.71 -1.63\n",
            " -0.68 -0.33 -1.61 -1.39 -1.79 -1.67 -1.71 -1.63 -2.28 -1.76 -1.61 -1.37\n",
            " -1.56 -1.4  -1.58 -0.27 -1.67 -1.56 -1.09 -1.7  -1.03 -1.53 -1.89 -1.38\n",
            " -1.81 -0.53 -1.69 -1.86 -1.8  -0.31 -0.87 -0.41 -2.16 -1.65 -2.1  -1.46\n",
            " -1.65 -0.44  0.11 -2.04  0.02 -1.22 -2.23 -1.9  -2.33 -0.04 -0.16 -1.17\n",
            " -2.13 -1.25 -1.78 -1.5  -0.11 -0.69 -0.9  -1.75 -1.68 -2.06 -1.45 -0.38\n",
            " -1.68 -1.96 -1.74 -0.07 -0.09 -0.96 -1.5  -0.33 -2.57 -1.39 -1.49 -1.59\n",
            " -1.3  -1.31 -0.63  0.06 -1.39 -0.4  -2.03 -1.75 -1.47 -1.35  0.09 -1.82\n",
            " -1.5  -1.4  -1.41 -0.53 -2.22 -0.6  -2.01 -0.61 -1.65 -1.27 -1.56 -0.12\n",
            " -1.74 -0.13 -1.49  0.12 -0.51 -2.3  -0.36 -0.48 -1.09 -0.45 -1.35 -1.56\n",
            " -2.48 -0.42 -1.56  0.1  -1.66 -0.42 -1.36 -1.73  0.01 -1.63 -1.79 -1.8\n",
            " -2.15 -1.82 -3.68 -0.29 -1.83 -1.47 -1.58 -0.47 -0.3  -1.73 -0.59 -1.43\n",
            " -0.04 -2.08 -1.93 -0.12 -0.45 -0.69 -1.44 -0.09 -1.54 -1.34 -1.4  -1.89\n",
            " -1.39 -1.77  0.02 -1.93 -2.16 -1.7  -0.1  -1.43 -1.74 -1.28 -0.76 -1.75\n",
            " -1.39 -1.19 -0.91 -0.82  0.06 -0.67 -1.28 -0.03 -0.87 -1.65 -2.14 -1.81\n",
            " -1.99 -1.45 -0.99 -1.32 -1.61 -1.39 -1.94 -1.06 -0.44 -1.23 -1.33 -1.31\n",
            " -1.71 -1.75 -1.75 -1.77 -0.11 -3.12 -1.5  -1.61 -1.56 -1.6  -1.48 -1.33\n",
            " -2.9  -1.75 -0.47 -0.99 -1.83 -1.66 -1.35 -0.45  0.05 -0.9  -1.14 -0.53\n",
            " -0.08 -0.59 -1.77 -1.74 -0.08 -2.04 -1.5  -1.71 -2.05 -1.63 -1.68  0.19\n",
            " -1.68 -2.12 -0.32 -1.83 -1.6  -1.52 -1.23 -1.81 -1.72 -0.09 -1.77 -1.42\n",
            " -1.42 -2.21 -1.72 -1.72 -1.49 -0.04 -1.4  -1.45  0.01 -1.31 -2.18 -1.77\n",
            " -1.94 -0.37 -1.09 -1.77 -1.94 -1.46 -1.89 -1.43 -0.21 -1.73 -1.4  -1.09\n",
            " -2.11 -1.88 -1.72  0.12 -1.35 -1.55 -1.66 -1.36 -2.18 -1.71 -1.7  -0.99\n",
            " -1.87 -1.82 -1.77 -1.92 -1.1  -1.56 -1.57 -1.74 -0.25 -1.65 -1.32 -0.01\n",
            " -0.09 -1.85 -1.7  -1.12 -1.88 -0.03 -0.6  -1.78 -0.18 -1.83 -1.53 -0.81\n",
            "  0.01 -1.92 -0.59 -2.86 -1.04 -0.86 -1.52 -1.71 -0.82 -1.68 -1.79 -0.14\n",
            " -1.68 -1.58 -1.59 -1.49 -1.7  -1.72 -1.69 -0.42 -0.58 -1.64 -1.45 -1.8\n",
            " -1.49 -2.03 -2.01 -0.46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**covariance_type 매개변수**\n",
        "* **\"full\"**\n",
        "  * 각 클러스터는 모양, 크기, 방향에 제약이 없습니다.\n",
        "* **\"spherical\"**\n",
        "  * 모든 클러스터가 원형입니다. 하지만 지름은 다를 수 있습니다.\n",
        "* **\"diag\"**\n",
        "  * 클러스터는 크기에 상관없이 어떤 타원형도 가능합니다. 하지만 타원의 축은 좌표축과 나란해야 합니다.\n",
        "* **\"tied\"**\n",
        "  * 모든 클러스터가 동일한 타원 모양, 크기, 방향을 가집니다."
      ],
      "metadata": {
        "id": "jAGrcx_O7o2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2.1 가우스 혼합을 사용한 이상치 탐지**"
      ],
      "metadata": {
        "id": "KdUbfoui-VOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가우스 혼합을 사용한 이상치 탐지**\n",
        "```\n",
        "가우스 혼합 모델을 이상치 탐지에 사용하는 방법은 매우 간단합니다.\n",
        "밀도가 낮은 지역에 있는 모든 샘플을 이상치로 볼 수 있습니다.\n",
        "만약 거짓 양성이 너무 많다면 임곗값을 낮추고 반대로 거짓 음성이 너무 많다면 임곗값을 더 높입니다.\n",
        "```"
      ],
      "metadata": {
        "id": "JgK8y1A7-bM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "densities = gm.score_samples(X)\n",
        "density_threshold = np.percentile(densities, 2)\n",
        "anomalies = X[densities < density_threshold]\n",
        "print(anomalies[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcry1gci_G-Y",
        "outputId": "71829252-18ca-4db9-ba2e-2e97a915687b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.22539919  0.90832444]\n",
            " [ 1.93139162  0.42163138]\n",
            " [-0.88678123  0.10686975]\n",
            " [ 1.96766534  0.52144872]\n",
            " [ 1.97812567  0.486804  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2.2 클러스터 개수 선택**"
      ],
      "metadata": {
        "id": "S9QeGd5K_lXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**이론적 정보 기준(BIC/AIC)**\n",
        "```\n",
        "k-평균에서는 이너셔나 실루엣 점수를 사용해 적절한 클러스터 개수를 선택합니다.\n",
        "가우스 혼합에서는 이런 지표를 사용할 수 없습니다.\n",
        "이런 지표들은 클러스터가 타원형이거나 크기가 다를 때 안정적이지 않기 때문입니다.\n",
        "```\n",
        "**[식 9-1] BIC와 AIC**\n",
        "$$BIC = log(m)p - 2log(\\hat{ℒ})$$\n",
        "$$AIC = 2p - 2log(\\hat{ℒ})$$\n",
        "* $m$은 샘플의 개수입니다.\n",
        "* $p$는 모델이 학습할 파라미터 개수입니다.\n",
        "* $\\hat{ℒ}$은 모델의 가능도 함수의 최댓값입니다."
      ],
      "metadata": {
        "id": "teA3zVWvALt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('gm.bic: {}'.format(gm.bic(X)))\n",
        "print('gm.aic: {}'.format(gm.aic(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2w8R4IWAx_w",
        "outputId": "8980fdde-94c3-4c7c-a92c-8df018e38278"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gm.bic: 2718.649829537455\n",
            "gm.aic: 2635.2179897947585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2.3 베이즈 가우스 혼합 모델**"
      ],
      "metadata": {
        "id": "Xn5BhnMPfoD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**베이즈 가우스 혼합 모델**\n",
        "```\n",
        "최적의 클러스터 개수를 수동으로 찾지 않고 불필요한 클러스터의 가중치를 0으로 만드는 BayesianGaussianMixture 클래스를 사용할 수 있습니다.\n",
        "클러스터 개수 n_components를 최적의 클러스터 개수보다 크다고 믿을 만한 값으로 지정합니다.\n",
        "이 알고리즘은 자동으로 불필요한 클러스터를 제거합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "SuOTXRlggXr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42).fit(X)\n",
        "print('bgm.weights_: {}'.format(bgm.weights_.round(2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aInMAsGlgx2Y",
        "outputId": "1aaa004e-1c24-4520-ca82-c473326b244b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bgm.weights_: [0.09 0.12 0.17 0.13 0.16 0.13 0.1  0.11 0.   0.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2.4 이상치 탐지와 특이치 탐지를 위한 알고리즘**"
      ],
      "metadata": {
        "id": "tRnrdxhZhJq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**사이킷런에는 이상치 탐지와 특이치 탐지 전용으로 사용할 수 있는 몇 가지 알고리즘이 구현되어 있습니다.**\n",
        "* **Fast-MCD**\n",
        "* **아이솔레이션 포레스트**\n",
        "* **LOF**\n",
        "* **one-class SVM**\n",
        "* **PCA**"
      ],
      "metadata": {
        "id": "7kBorIdTh8Uf"
      }
    }
  ]
}